#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
This file is automatically generated by AION for AION_37_1 usecase.
File generation time: 2022-07-19 11:45:42
'''
#Standard Library modules
import logging
import sys
import json
import time
import platform
import tempfile
import shutil
import argparse
import sklearn
import pandas as pd
from pathlib import Path
#Third Party modules
from pathlib import Path

input_file = {
"trainingData":"rawData.dat",
"performance": "performance.json",
"prodGrndTruData":"prodGrndTruData.dat",
"prodData":"prodData.dat" 
}

def read_json(file_path):                    
    data = None                    
    with open(file_path,'r') as f:                    
        data = json.load(f)                    
    return data                    

def is_drift_within_limits(train_matrices, current_matrices, threshold = 5):
    scoring_criteria = train_matrices['scoring_criteria']
    train_score = train_matrices['metrices']['test_score']/100
    current_score = current_matrices[scoring_criteria]
    threshold_value = train_score * threshold / 100.0
    if current_score > (train_score - threshold_value) :
        return True
    else:
        return False

def get_metrices(actual_values, predicted_values):        
    from sklearn.metrics import accuracy_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import f1_score
    result = {} 
    accuracy_score = accuracy_score(actual_values, predicted_values)        
    avg_precision = precision_score(actual_values, predicted_values,        
        average='macro')        
    avg_recall = recall_score(actual_values, predicted_values,        
        average='macro')        
    avg_f1 = f1_score(actual_values, predicted_values,        
        average='macro')        
        
    result['accuracy'] = accuracy_score        
    result['precision'] = avg_precision        
    result['recall'] = avg_recall        
    result['f1'] = avg_f1        
    return result            
            	
def monitoring(config):
    from input_drift import inputdrift
    output_json = {}
    trainingDataLocation = Path(config['trainingDataPath'])/input_file['trainingData']
    trainingStatus = 'False'
    if trainingDataLocation.exists(): 	
        actual_data_path = Path(config['prodDataPath'])/input_file['prodGrndTruData']
        predict_data_path = Path(config['prodDataPath'])/input_file['prodData']
        performance= Path(config['prodDataPath'])/input_file['performance']
        if performance.exists():        
            performance = read_json(performance)  
            if actual_data_path.exists() and predict_data_path.exists():
                predicted_data = pd.read_csv(predict_data_path)        		
                actual_data_path = pd.read_csv(actual_data_path)
                common_col = [k for k in predicted_data.columns.tolist() if k in actual_data_path.columns.tolist()]				
                mergedRes = pd.merge(actual_data_path, predicted_data, on =common_col,how = 'inner')
                currentPerformance = {} 			
                currentPerformance = get_metrices(mergedRes['actual'], mergedRes['prediction'])
                if is_drift_within_limits(performance, currentPerformance):
                    output_json.update({'outputDrift':'Model score is with in limits'})
                else:
                    output_json.update({'status':'SUCCESS','outputDrift':{'Meassage': 'Model output is drifted','trainedScore':performance['metrices']['test_score'], 'currentScore':currentPerformance[performance['scoring_criteria']]}})
                    trainingStatus = 'True'
            else:
                output_json.update({'outputDrift':'Prod Data not found'}) 
        else:
            output_json.update({'status':'SUCCESS','Msg':'Pipeline is not executed completely'})
            trainingStatus = 'True'
				
			
        if trainingStatus == 'False':	       			
            historicaldataFrame=pd.read_csv(trainingDataLocation)        
            currentdataFrame=pd.read_csv(config['inputDataLocation']) 	
            inputdriftObj = inputdrift(config)
            dataalertcount,inputdrift_message = inputdriftObj.get_input_drift(currentdataFrame,historicaldataFrame)	

            if inputdrift_message == 'Model is working as expected':        
                output_json.update({'status':'SUCCESS','inputDrift':'Model is working as expected'})        
            else:        
                output_json.update({'status':'SUCCESS','inputDrift':{'Affected Columns':inputdrift_message}})        
                trainingStatus = 'True'
    else:
        output_json.update({'status':'SUCCESS','Msg':'Pipeline executing first time'}) 	
    with open(config['driftStatusLocation'], 'w') as statusFile:
        statusFile.write(str(trainingStatus))
    return(json.dumps(output_json))        
        
	
if __name__ == '__main__':        
    parser = argparse.ArgumentParser()        
    parser.add_argument('-l', '--inputDataLocation', help='Training Data Location')
    parser.add_argument('-t', '--trainingDataPath', help='Raw Data Folder Path')	
    parser.add_argument('-p', '--prodDataPath', help='Production Folder Path')
    parser.add_argument('-d', '--driftStatusLocation', help='Monitoring Status Location')	
    args = parser.parse_args()        
        
    config = {'inputDataLocation':None,'trainingDataPath':None,'predictDataPath':None,'driftStatusLocation':'trainRequired'}        
    config_file = Path(__file__).parent/'config.json'        
    if not Path(config_file).exists():        
        raise ValueError(f'Config file is missing: {config_file}')        
    config = read_json(config_file) 
	
    if args.inputDataLocation:        
        config['inputDataLocation'] = args.inputDataLocation
    if args.trainingDataPath:        
        config['trainingDataPath'] = args.trainingDataPath
    if args.prodDataPath:        
        config['prodDataPath'] = args.prodDataPath
    if args.driftStatusLocation:        
        config['driftStatusLocation'] = args.driftStatusLocation
        
    try:        
        print(monitoring(config))        
    except Exception as e:        
        status = {'Status':'Failure','Message':str(e)}        
        print(json.dumps(status))
